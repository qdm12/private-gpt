services:
  private-gpt:
    image: privategpt
    build:
      context: .
      # context: https://github.com/qdm12/private-gpt.git#docker-tip
      # TODO: change to https://github.com/zylon-api/private-gpt.git#main
      args:
        - POETRY_EXTRAS=vector-stores-qdrant llms-ollama embeddings-ollama embeddings-huggingface
    volumes:
      - ./local_data/:/home/worker/app/local_data
      - ./cache/tiktoken:/home/worker/app/tiktoken_cache
      - ./cache/models:/home/worker/app/models/cache
    ports:
      - 8001:8080
    environment:
      PGPT_MODE: ollama
      PGPT_EMBEDDING_MODE: mock
      PGPT_HF_REPO_ID: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
      PGPT_HF_MODEL_FILE: mistral-7b-instruct-v0.1.Q4_K_M.gguf
      PGPT_EMBEDDING_HF_MODEL_NAME: BAAI/bge-small-en-v1.5
      PGPT_SAGEMAKER_LLM_ENDPOINT_NAME:
      PGPT_SAGEMAKER_EMBEDDING_ENDPOINT_NAME:
      PGPT_OLLAMA_LLM_MODEL: mistral
      PGPT_OLLAMA_EMBEDDING_MODEL: nomic-embed-text
      PGPT_OLLAMA_API_BASE: http://ollama:11434
      PGPT_OLLAMA_TFS_Z: 1.0
      PGPT_OLLAMA_TOP_K: 40
      PGPT_OLLAMA_TOP_P: 0.9
      PGPT_OLLAMA_REPEAT_LAST_N: 64
      PGPT_OLLAMA_REPEAT_PENALTY: 1.2
      PGPT_OLLAMA_REQUEST_TIMEOUT: 600.0
      PGPT_OPENAI_API_BASE: https://api.openai.com/v1
      PGPT_OPENAI_API_KEY: EMPTY
      PGPT_OPENAI_MODEL: ""
      PGPT_RAG_SIMILARITY_TOP_K: 2
      PGPT_RAG_RERANK_ENABLED: false
      PGPT_RAG_RERANK_MODEL: cross-encoder/ms-marco-MiniLM-L-2-v2
      PGPT_RAG_RERANK_TOP_N: 2
    ## Nvidia  GPU support ##
    # runtime: nvidia
    # ipc: host
    # devices:
    #   - nvidia.com/gpu=0
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ./models:/root/.ollama
    expose:
      - 11434
